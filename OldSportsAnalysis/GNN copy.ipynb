{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8665539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv,GATConv\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, classification_report\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import mediapipe as mp\n",
    "from pymongo import MongoClient\n",
    "from neo4j import GraphDatabase\n",
    "from collections import defaultdict\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4914e551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=2,  \n",
    "    smooth_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e7e2a04",
   "metadata": {},
   "outputs": [],
   "source": [
    " #MongoDB connection\n",
    "mongo_client = MongoClient(\"mongodb://admin:password@localhost:27017/\")\n",
    "mongo_db = mongo_client[\"SportsAnalysis\"]\n",
    "labels_collection = mongo_db[\"metadata\"]\n",
    "\n",
    "# Neo4j connection\n",
    "neo4j_driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc1a7677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_video_ids():\n",
    "        return [doc['video_id'] for doc in labels_collection.find({}, {'video_id': 1})]\n",
    "\n",
    "def fetch_label(video_id):\n",
    "        doc = labels_collection.find_one({'video_id': video_id})\n",
    "        return doc['label'] if doc else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d6c45e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_graphs_from_neo4j(video_id):\n",
    "        with neo4j_driver.session() as session:\n",
    "            # Get all unique time steps\n",
    "            result = session.run(\"\"\"\n",
    "                MATCH (n:PoseNode {video_id: $video_id})\n",
    "                RETURN DISTINCT n.time_index AS timestep\n",
    "                ORDER BY timestep ASC\n",
    "            \"\"\", video_id=video_id)\n",
    "            time_steps = [record[\"timestep\"] for record in result]\n",
    "\n",
    "            graphs = []\n",
    "            for t in time_steps:\n",
    "                # Fetch nodes\n",
    "                node_query = session.run(\"\"\"\n",
    "                    MATCH (n:PoseNode {video_id: $video_id, time_index: $t})\n",
    "                    RETURN n.node_index AS idx, n.angle AS angle, n.time AS time\n",
    "                    ORDER BY idx\n",
    "                \"\"\", video_id=video_id, t=t)\n",
    "\n",
    "                node_data = []\n",
    "                time_value = 0\n",
    "                for record in node_query:\n",
    "                    node_data.append(float(record[\"angle\"]))\n",
    "                    time_value = float(record[\"time\"])\n",
    "\n",
    "                x = torch.tensor(node_data, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "                # Fetch edges\n",
    "                edge_query = session.run(\"\"\"\n",
    "                    MATCH (a:PoseNode {video_id: $video_id, time_index: $t})-[r:CONNECTED_TO]->(b:PoseNode)\n",
    "                    RETURN a.node_index AS src, b.node_index AS dst, r.weight AS weight\n",
    "                \"\"\", video_id=video_id, t=t)\n",
    "\n",
    "                edge_index = []\n",
    "                edge_attr = []\n",
    "                for record in edge_query:\n",
    "                    edge_index.append([int(record[\"src\"]), int(record[\"dst\"])])\n",
    "                    edge_attr.append([float(record[\"weight\"])])\n",
    "\n",
    "                edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "                edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "\n",
    "                graphs.append({\n",
    "                    \"edge_index\": edge_index,\n",
    "                    \"edge_attr\": edge_attr,\n",
    "                    \"angle_features\": x,\n",
    "                    \"time\": time_value,\n",
    "                    \"source_video\": video_id,\n",
    "                    \"label\": fetch_label(video_id),\n",
    "                    \"node_mapping\": {},  # Optional: mapping if you have remapped indices\n",
    "                    \"reverse_mapping\": {},\n",
    "                    \"node_features\": x.clone()\n",
    "                })\n",
    "            return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffa79a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph_sequences_from_db():\n",
    "    video_ids = fetch_all_video_ids()\n",
    "    all_data = []\n",
    "\n",
    "    for vid in video_ids:\n",
    "        try:\n",
    "            graph_sequence = fetch_graphs_from_neo4j(vid)\n",
    "            if graph_sequence:\n",
    "                all_data.append(graph_sequence)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading video {vid}: {e}\")\n",
    "\n",
    "    print(f\"✅ Loaded {len(all_data)} videos from DB\")\n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db5c46a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "class FreeThrowDataset(Dataset):\n",
    "    def __init__(self, matrix_data):\n",
    "        self.data = matrix_data\n",
    "        # The angle nodes are automatically mapped in our enhanced processing\n",
    "        # But we can still keep track of which landmarks are associated with angles\n",
    "        self.angle_node_landmarks = [\n",
    "            mp_pose.PoseLandmark.LEFT_ELBOW.value,\n",
    "            mp_pose.PoseLandmark.RIGHT_ELBOW.value,\n",
    "            mp_pose.PoseLandmark.LEFT_SHOULDER.value,\n",
    "            mp_pose.PoseLandmark.RIGHT_SHOULDER.value,\n",
    "            mp_pose.PoseLandmark.LEFT_KNEE.value,\n",
    "            mp_pose.PoseLandmark.RIGHT_KNEE.value,\n",
    "            mp_pose.PoseLandmark.LEFT_HIP.value,\n",
    "            mp_pose.PoseLandmark.RIGHT_HIP.value\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.data[idx]\n",
    "        data_sequence = []\n",
    "        \n",
    "        for timestep in sequence:\n",
    "            # Check if timestep is a dictionary (as expected)\n",
    "            if not isinstance(timestep, dict):\n",
    "                raise TypeError(f\"Expected dictionary, got {type(timestep)}. Value: {timestep}\")\n",
    "                \n",
    "            try:\n",
    "                edge_index = timestep['edge_index']\n",
    "                edge_attr = timestep['edge_attr']\n",
    "                label = timestep['label']\n",
    "                angle_features = timestep['angle_features']\n",
    "                \n",
    "                # Convert label to tensor if it's not already\n",
    "                if not isinstance(label, torch.Tensor):\n",
    "                    y = torch.tensor([label], dtype=torch.float)\n",
    "                else:\n",
    "                    y = label\n",
    "                \n",
    "                # Create graph data object with already remapped indices and features\n",
    "                data = Data(\n",
    "                    x=angle_features,  # Use angle features as node features\n",
    "                    edge_index=edge_index,\n",
    "                    edge_attr=edge_attr,\n",
    "                    y=y,\n",
    "                    num_nodes=angle_features.size(0)\n",
    "                )\n",
    "                \n",
    "                # Store additional information for reference\n",
    "                data.original_to_new_mapping = timestep['node_mapping']\n",
    "                data.new_to_original_mapping = timestep['reverse_mapping']\n",
    "                data.positional_features = timestep['node_features']  # Store original position features\n",
    "                data.time = timestep['time']\n",
    "                data.source_video = timestep['source_video']\n",
    "                \n",
    "                data_sequence.append(data)\n",
    "            except KeyError as e:\n",
    "                # Print detailed error info for debugging\n",
    "                print(f\"KeyError: {e} not found in timestep. Available keys: {list(timestep.keys())}\")\n",
    "                raise\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing timestep: {e}\")\n",
    "                raise\n",
    "            \n",
    "        return data_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for batching sequences.\n",
    "    Each batch item is a sequence of frames, and we want to\n",
    "    maintain these sequences.\n",
    "    \"\"\"\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5713fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(graphdata, train_ratio=0.7, random_seed=42):\n",
    "    \"\"\"\n",
    "    Split graph data while maintaining class distribution in both train and test sets.\n",
    "    \n",
    "    Args:\n",
    "        graphdata: List of graph sequences\n",
    "        train_ratio: Proportion of data for training (default: 0.7)\n",
    "        random_seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        train_matrix, test_matrix: Stratified splits of the data\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Group data by labels\n",
    "    label_to_sequences = defaultdict(list)\n",
    "    \n",
    "    for sequence in graphdata:\n",
    "        # Get label from first timestep (all timesteps in a sequence have same label)\n",
    "        if sequence and len(sequence) > 0:\n",
    "            label = sequence[0]['label']\n",
    "            label_to_sequences[label].append(sequence)\n",
    "    \n",
    "    # Print class distribution\n",
    "    print(\"Class distribution in dataset:\")\n",
    "    for label, sequences in label_to_sequences.items():\n",
    "        print(f\"  Class {label}: {len(sequences)} sequences ({len(sequences)/len(graphdata)*100:.1f}%)\")\n",
    "    \n",
    "    train_matrix = []\n",
    "    test_matrix = []\n",
    "    \n",
    "    # Split each class proportionally\n",
    "    for label, sequences in label_to_sequences.items():\n",
    "        # Shuffle sequences for this class\n",
    "        random.shuffle(sequences)\n",
    "        \n",
    "        # Calculate split point\n",
    "        n_train = int(len(sequences) * train_ratio)\n",
    "        \n",
    "        # Ensure at least one sample in test if possible\n",
    "        if len(sequences) > 1 and n_train == len(sequences):\n",
    "            n_train = len(sequences) - 1\n",
    "        \n",
    "        # Split the sequences\n",
    "        train_sequences = sequences[:n_train]\n",
    "        test_sequences = sequences[n_train:]\n",
    "        \n",
    "        train_matrix.extend(train_sequences)\n",
    "        test_matrix.extend(test_sequences)\n",
    "        \n",
    "        print(f\"  Class {label}: {len(train_sequences)} train, {len(test_sequences)} test\")\n",
    "    \n",
    "    # Final shuffle to mix classes\n",
    "    random.shuffle(train_matrix)\n",
    "    random.shuffle(test_matrix)\n",
    "    \n",
    "    print(f\"\\nFinal split: {len(train_matrix)} train, {len(test_matrix)} test\")\n",
    "    \n",
    "    # Verify class distribution is maintained\n",
    "    train_labels = [seq[0]['label'] for seq in train_matrix if seq]\n",
    "    test_labels = [seq[0]['label'] for seq in test_matrix if seq]\n",
    "    \n",
    "    print(f\"\\nTrain set class distribution:\")\n",
    "    train_counter = Counter(train_labels)\n",
    "    for label, count in sorted(train_counter.items()):\n",
    "        print(f\"  Class {label}: {count} ({count/len(train_labels)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nTest set class distribution:\")\n",
    "    test_counter = Counter(test_labels)\n",
    "    for label, count in sorted(test_counter.items()):\n",
    "        print(f\"  Class {label}: {count} ({count/len(test_labels)*100:.1f}%)\")\n",
    "    \n",
    "    return train_matrix, test_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cf32de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stratified_datasets(graphdata, train_ratio=0.7, random_seed=42):\n",
    "    \"\"\"\n",
    "    Create stratified train/test datasets with proper class balance.\n",
    "    \"\"\"\n",
    "    train_matrix, test_matrix = stratified_split(graphdata, train_ratio, random_seed)\n",
    "    \n",
    "    train_dataset = FreeThrowDataset(train_matrix)\n",
    "    test_dataset = FreeThrowDataset(test_matrix)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    return train_dataset, test_dataset, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2ae6874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_LSTM(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, lstm_hidden, num_classes):\n",
    "        super().__init__()\n",
    "        self.gcn = GCNConv(in_channels, hidden_channels)\n",
    "        self.lstm = nn.LSTM(hidden_channels, lstm_hidden, batch_first=True)\n",
    "        self.classifier = nn.Linear(lstm_hidden, num_classes)\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        gcn_outputs = []\n",
    "        for data in sequence:\n",
    "            x = self.gcn(data.x, data.edge_index)\n",
    "            x = torch.relu(x)\n",
    "            pooled = x.mean(dim=0)  # Global mean pooling\n",
    "            gcn_outputs.append(pooled)\n",
    "\n",
    "        gcn_outputs = torch.stack(gcn_outputs).unsqueeze(0)  # [1, T, F]\n",
    "        lstm_out, _ = self.lstm(gcn_outputs)\n",
    "        out = self.classifier(lstm_out[:, -1, :])  # Use last time step\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb3f0b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_LSTM_SpatialAttention(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, lstm_hidden, num_classes, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.gat = GATConv(in_channels, hidden_channels, heads=num_heads, concat=False, dropout=0.1)\n",
    "        self.lstm = nn.LSTM(hidden_channels, lstm_hidden, batch_first=True)\n",
    "        self.classifier = nn.Linear(lstm_hidden, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        gcn_outputs = []\n",
    "        for data in sequence:\n",
    "            x = self.gat(data.x, data.edge_index)\n",
    "            x = torch.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            pooled = x.mean(dim=0)  # Global mean pooling\n",
    "            gcn_outputs.append(pooled)\n",
    "\n",
    "        gcn_outputs = torch.stack(gcn_outputs).unsqueeze(0)  # [1, T, F]\n",
    "        lstm_out, _ = self.lstm(gcn_outputs)\n",
    "        out = self.classifier(lstm_out[:, -1, :])  # Use last time step\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0b34a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, lstm_output):\n",
    "        # Compute attention weights for each time step\n",
    "        attn_weights = self.attention(lstm_output)  # [batch_size, seq_len, 1]\n",
    "        attn_weights = F.softmax(attn_weights, dim=1)  # Normalize over time\n",
    "        \n",
    "        # Weighted sum over time dimension\n",
    "        attended_output = torch.sum(lstm_output * attn_weights, dim=1)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        return attended_output, attn_weights.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f769291",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(input_dim // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Compute attention weights for each node\n",
    "        attn_weights = self.attention(x)  # [num_nodes, 1]\n",
    "        attn_weights = F.softmax(attn_weights, dim=0)  # Normalize\n",
    "        \n",
    "        # Weighted sum of node features\n",
    "        pooled = torch.sum(x * attn_weights, dim=0)  # [input_dim]\n",
    "        return pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffb4236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_LSTM_TemporalAttention(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, lstm_hidden, num_classes):\n",
    "        super().__init__()\n",
    "        self.gcn = GCNConv(in_channels, hidden_channels)\n",
    "        self.lstm = nn.LSTM(hidden_channels, lstm_hidden, batch_first=True)\n",
    "        self.temporal_attention = TemporalAttention(lstm_hidden)\n",
    "        self.classifier = nn.Linear(lstm_hidden, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        gcn_outputs = []\n",
    "        for data in sequence:\n",
    "            x = self.gcn(data.x, data.edge_index)\n",
    "            x = torch.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            pooled = x.mean(dim=0)\n",
    "            gcn_outputs.append(pooled)\n",
    "\n",
    "        gcn_outputs = torch.stack(gcn_outputs).unsqueeze(0)  # [1, T, F]\n",
    "        lstm_out, _ = self.lstm(gcn_outputs)\n",
    "        \n",
    "        # Apply temporal attention instead of using just last time step\n",
    "        attended_output, attention_weights = self.temporal_attention(lstm_out)\n",
    "        out = self.classifier(attended_output)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "546fd2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_LSTM_FullAttention(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, lstm_hidden, num_classes, num_heads=4):\n",
    "        super().__init__()\n",
    "        # Spatial attention with GAT\n",
    "        self.gat = GATConv(in_channels, hidden_channels, heads=num_heads, concat=False, dropout=0.1)\n",
    "        \n",
    "        # Attention pooling\n",
    "        self.attention_pool = AttentionPooling(hidden_channels)\n",
    "        \n",
    "        # LSTM with temporal attention\n",
    "        self.lstm = nn.LSTM(hidden_channels, lstm_hidden, batch_first=True)\n",
    "        self.temporal_attention = TemporalAttention(lstm_hidden)\n",
    "        \n",
    "        # Classification\n",
    "        self.classifier = nn.Linear(lstm_hidden, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_channels)\n",
    "        self.layer_norm2 = nn.LayerNorm(lstm_hidden)\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        gcn_outputs = []\n",
    "        \n",
    "        for data in sequence:\n",
    "            # Spatial attention with GAT\n",
    "            x = self.gat(data.x, data.edge_index)\n",
    "            x = self.layer_norm1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "            # Attention-based pooling\n",
    "            pooled = self.attention_pool(x)\n",
    "            gcn_outputs.append(pooled)\n",
    "\n",
    "        gcn_outputs = torch.stack(gcn_outputs).unsqueeze(0)  # [1, T, F]\n",
    "        \n",
    "        # LSTM processing\n",
    "        lstm_out, _ = self.lstm(gcn_outputs)\n",
    "        lstm_out = self.layer_norm2(lstm_out)\n",
    "        \n",
    "        # Temporal attention\n",
    "        attended_output, temporal_attention_weights = self.temporal_attention(lstm_out)\n",
    "        \n",
    "        # Final classification\n",
    "        out = self.classifier(attended_output)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f293e3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 88 videos from DB\n"
     ]
    }
   ],
   "source": [
    "graphdata=load_graph_sequences_from_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e27439",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(graphdata)\n",
    "split = int(0.7 * len(graphdata))\n",
    "\n",
    "train_matrix = graphdata[:split]\n",
    "test_matrix = graphdata[split:]\n",
    "\n",
    "train_dataset = FreeThrowDataset(train_matrix)\n",
    "test_dataset = FreeThrowDataset(test_matrix)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf14209",
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ef8ff0",
   "metadata": {},
   "source": [
    "# TGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d86df277",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN_LSTM(in_channels=1, hidden_channels=32, lstm_hidden=16, num_classes=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e56790f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 2.5310\n",
      "Epoch 2 - Loss: 2.5680\n",
      "Epoch 3 - Loss: 2.2426\n",
      "Epoch 4 - Loss: 2.4849\n",
      "Epoch 5 - Loss: 2.0494\n",
      "Epoch 6 - Loss: 2.4303\n",
      "Epoch 7 - Loss: 1.9252\n",
      "Epoch 8 - Loss: 2.3879\n",
      "Epoch 9 - Loss: 1.8158\n",
      "Epoch 10 - Loss: 2.3547\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        sequence = batch[0]  # batch size = 1\n",
    "        target = sequence[0].y\n",
    "        output = model(sequence)\n",
    "        loss = loss_fn(output.view(-1), target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "329d4b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (manual): 21/27 = 77.78%\n",
      "Accuracy (sklearn): 0.7778\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.8750\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         6\n",
      "           1       0.78      1.00      0.88        21\n",
      "\n",
      "    accuracy                           0.78        27\n",
      "   macro avg       0.39      0.50      0.44        27\n",
      "weighted avg       0.60      0.78      0.68        27\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayman\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ayman\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ayman\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        sequence = batch[0]\n",
    "        target = int(sequence[0].y.item())\n",
    "        output = model(sequence)\n",
    "        prediction = (torch.sigmoid(output) > 0.5).int().item()\n",
    "        \n",
    "        correct += int(prediction == target)\n",
    "        total += 1\n",
    "        \n",
    "        all_predictions.append(prediction)\n",
    "        all_targets.append(target)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_manual = correct / total\n",
    "accuracy = accuracy_score(all_targets, all_predictions)\n",
    "recall = recall_score(all_targets, all_predictions, average='binary')  # for binary classification\n",
    "f1 = f1_score(all_targets, all_predictions, average='binary')  # for binary classification\n",
    "\n",
    "# Print results\n",
    "print(f\"Test Accuracy (manual): {correct}/{total} = {accuracy_manual:.2%}\")\n",
    "print(f\"Accuracy (sklearn): {accuracy:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Optional: Print detailed classification report\n",
    "print('\\nDetailed Classification Report:')\n",
    "print(classification_report(all_targets, all_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d63105",
   "metadata": {},
   "source": [
    "# GNN Spatial Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "680d1341",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN_LSTM_SpatialAttention(in_channels=1, hidden_channels=32, lstm_hidden=16, num_classes=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4de6608a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 2.9093\n",
      "Epoch 2 - Loss: 2.7949\n",
      "Epoch 3 - Loss: 2.8416\n",
      "Epoch 4 - Loss: 2.8256\n",
      "Epoch 5 - Loss: 2.7717\n",
      "Epoch 6 - Loss: 2.7873\n",
      "Epoch 7 - Loss: 3.0595\n",
      "Epoch 8 - Loss: 2.7994\n",
      "Epoch 9 - Loss: 2.8233\n",
      "Epoch 10 - Loss: 2.7733\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        sequence = batch[0]  # batch size = 1\n",
    "        target = sequence[0].y\n",
    "        output = model(sequence)\n",
    "        loss = loss_fn(output.view(-1), target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ddf46a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (manual): 20/27 = 74.07%\n",
      "Accuracy (sklearn): 0.7407\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.8511\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         7\n",
      "           1       0.74      1.00      0.85        20\n",
      "\n",
      "    accuracy                           0.74        27\n",
      "   macro avg       0.37      0.50      0.43        27\n",
      "weighted avg       0.55      0.74      0.63        27\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayman\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ayman\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ayman\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        sequence = batch[0]\n",
    "        target = int(sequence[0].y.item())\n",
    "        output = model(sequence)\n",
    "        prediction = (torch.sigmoid(output) > 0.5).int().item()\n",
    "        \n",
    "        correct += int(prediction == target)\n",
    "        total += 1\n",
    "        \n",
    "        all_predictions.append(prediction)\n",
    "        all_targets.append(target)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_manual = correct / total\n",
    "accuracy = accuracy_score(all_targets, all_predictions)\n",
    "recall = recall_score(all_targets, all_predictions, average='binary')  # for binary classification\n",
    "f1 = f1_score(all_targets, all_predictions, average='binary')  # for binary classification\n",
    "\n",
    "# Print results\n",
    "print(f\"Test Accuracy (manual): {correct}/{total} = {accuracy_manual:.2%}\")\n",
    "print(f\"Accuracy (sklearn): {accuracy:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Optional: Print detailed classification report\n",
    "print('\\nDetailed Classification Report:')\n",
    "print(classification_report(all_targets, all_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a2b275",
   "metadata": {},
   "source": [
    "# GNN Temporal Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "15a9e163",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN_LSTM_TemporalAttention(in_channels=1, hidden_channels=32, lstm_hidden=16, num_classes=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2603c67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 2.9727\n",
      "Epoch 2 - Loss: 2.8614\n",
      "Epoch 3 - Loss: 2.7553\n",
      "Epoch 4 - Loss: 2.5149\n",
      "Epoch 5 - Loss: 2.5345\n",
      "Epoch 6 - Loss: 2.4570\n",
      "Epoch 7 - Loss: 2.4456\n",
      "Epoch 8 - Loss: 1.8425\n",
      "Epoch 9 - Loss: 2.8563\n",
      "Epoch 10 - Loss: 1.6643\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        sequence = batch[0]  # batch size = 1\n",
    "        target = sequence[0].y\n",
    "        output = model(sequence)\n",
    "        loss = loss_fn(output.view(-1), target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "65259dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (manual): 22/27 = 81.48%\n",
      "Accuracy (sklearn): 0.8148\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.8980\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.81      1.00      0.90        22\n",
      "\n",
      "    accuracy                           0.81        27\n",
      "   macro avg       0.41      0.50      0.45        27\n",
      "weighted avg       0.66      0.81      0.73        27\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayman\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ayman\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ayman\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        sequence = batch[0]\n",
    "        target = int(sequence[0].y.item())\n",
    "        output = model(sequence)\n",
    "        prediction = (torch.sigmoid(output) > 0.5).int().item()\n",
    "        \n",
    "        correct += int(prediction == target)\n",
    "        total += 1\n",
    "        \n",
    "        all_predictions.append(prediction)\n",
    "        all_targets.append(target)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_manual = correct / total\n",
    "accuracy = accuracy_score(all_targets, all_predictions)\n",
    "recall = recall_score(all_targets, all_predictions, average='binary')  # for binary classification\n",
    "f1 = f1_score(all_targets, all_predictions, average='binary')  # for binary classification\n",
    "\n",
    "# Print results\n",
    "print(f\"Test Accuracy (manual): {correct}/{total} = {accuracy_manual:.2%}\")\n",
    "print(f\"Accuracy (sklearn): {accuracy:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Optional: Print detailed classification report\n",
    "print('\\nDetailed Classification Report:')\n",
    "print(classification_report(all_targets, all_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bbe692",
   "metadata": {},
   "source": [
    "# GNN Temporal + Spatial Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c2c4d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN_LSTM_FullAttention(in_channels=1, hidden_channels=32, lstm_hidden=16, num_classes=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "711921cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 2.8795\n",
      "Epoch 2 - Loss: 1.2333\n",
      "Epoch 3 - Loss: 2.2943\n",
      "Epoch 4 - Loss: 3.9728\n",
      "Epoch 5 - Loss: 0.7337\n",
      "Epoch 6 - Loss: 0.7265\n",
      "Epoch 7 - Loss: 3.9687\n",
      "Epoch 8 - Loss: 0.7352\n",
      "Epoch 9 - Loss: 0.7257\n",
      "Epoch 10 - Loss: 0.6826\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        sequence = batch[0]  # batch size = 1\n",
    "        target = sequence[0].y\n",
    "        output = model(sequence)\n",
    "        loss = loss_fn(output.view(-1), target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "567cd5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (manual): 20/27 = 74.07%\n",
      "Accuracy (sklearn): 0.7407\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.8511\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         7\n",
      "           1       0.74      1.00      0.85        20\n",
      "\n",
      "    accuracy                           0.74        27\n",
      "   macro avg       0.37      0.50      0.43        27\n",
      "weighted avg       0.55      0.74      0.63        27\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayman\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ayman\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ayman\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        sequence = batch[0]\n",
    "        target = int(sequence[0].y.item())\n",
    "        output = model(sequence)\n",
    "        prediction = (torch.sigmoid(output) > 0.5).int().item()\n",
    "        \n",
    "        correct += int(prediction == target)\n",
    "        total += 1\n",
    "        \n",
    "        all_predictions.append(prediction)\n",
    "        all_targets.append(target)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_manual = correct / total\n",
    "accuracy = accuracy_score(all_targets, all_predictions)\n",
    "recall = recall_score(all_targets, all_predictions, average='binary')  # for binary classification\n",
    "f1 = f1_score(all_targets, all_predictions, average='binary')  # for binary classification\n",
    "\n",
    "# Print results\n",
    "print(f\"Test Accuracy (manual): {correct}/{total} = {accuracy_manual:.2%}\")\n",
    "print(f\"Accuracy (sklearn): {accuracy:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Optional: Print detailed classification report\n",
    "print('\\nDetailed Classification Report:')\n",
    "print(classification_report(all_targets, all_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
