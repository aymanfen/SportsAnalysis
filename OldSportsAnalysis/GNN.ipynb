{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8665539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv,GATConv\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, classification_report\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import mediapipe as mp\n",
    "from pymongo import MongoClient\n",
    "from neo4j import GraphDatabase\n",
    "from collections import defaultdict\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4914e551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=2,  \n",
    "    smooth_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e7e2a04",
   "metadata": {},
   "outputs": [],
   "source": [
    " #MongoDB connection\n",
    "mongo_client = MongoClient(\"mongodb://admin:password@localhost:27017/\")\n",
    "mongo_db = mongo_client[\"SportsAnalysis\"]\n",
    "labels_collection = mongo_db[\"metadata\"]\n",
    "\n",
    "# Neo4j connection\n",
    "neo4j_driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc1a7677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_video_ids():\n",
    "        return [doc['video_id'] for doc in labels_collection.find({}, {'video_id': 1})]\n",
    "\n",
    "def fetch_label(video_id):\n",
    "        doc = labels_collection.find_one({'video_id': video_id})\n",
    "        return doc['label'] if doc else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d6c45e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_graphs_from_neo4j(video_id):\n",
    "        with neo4j_driver.session() as session:\n",
    "            # Get all unique time steps\n",
    "            result = session.run(\"\"\"\n",
    "                MATCH (n:PoseNode {video_id: $video_id})\n",
    "                RETURN DISTINCT n.time_index AS timestep\n",
    "                ORDER BY timestep ASC\n",
    "            \"\"\", video_id=video_id)\n",
    "            time_steps = [record[\"timestep\"] for record in result]\n",
    "\n",
    "            graphs = []\n",
    "            for t in time_steps:\n",
    "                # Fetch nodes\n",
    "                node_query = session.run(\"\"\"\n",
    "                    MATCH (n:PoseNode {video_id: $video_id, time_index: $t})\n",
    "                    RETURN n.node_index AS idx, n.angle AS angle, n.time AS time\n",
    "                    ORDER BY idx\n",
    "                \"\"\", video_id=video_id, t=t)\n",
    "\n",
    "                node_data = []\n",
    "                time_value = 0\n",
    "                for record in node_query:\n",
    "                    node_data.append(float(record[\"angle\"]))\n",
    "                    time_value = float(record[\"time\"])\n",
    "\n",
    "                x = torch.tensor(node_data, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "                # Fetch edges\n",
    "                edge_query = session.run(\"\"\"\n",
    "                    MATCH (a:PoseNode {video_id: $video_id, time_index: $t})-[r:CONNECTED_TO]->(b:PoseNode)\n",
    "                    RETURN a.node_index AS src, b.node_index AS dst, r.weight AS weight\n",
    "                \"\"\", video_id=video_id, t=t)\n",
    "\n",
    "                edge_index = []\n",
    "                edge_attr = []\n",
    "                for record in edge_query:\n",
    "                    edge_index.append([int(record[\"src\"]), int(record[\"dst\"])])\n",
    "                    edge_attr.append([float(record[\"weight\"])])\n",
    "\n",
    "                edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "                edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "\n",
    "                graphs.append({\n",
    "                    \"edge_index\": edge_index,\n",
    "                    \"edge_attr\": edge_attr,\n",
    "                    \"angle_features\": x,\n",
    "                    \"time\": time_value,\n",
    "                    \"source_video\": video_id,\n",
    "                    \"label\": fetch_label(video_id),\n",
    "                    \"node_mapping\": {},  # Optional: mapping if you have remapped indices\n",
    "                    \"reverse_mapping\": {},\n",
    "                    \"node_features\": x.clone()\n",
    "                })\n",
    "            return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffa79a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph_sequences_from_db():\n",
    "    video_ids = fetch_all_video_ids()\n",
    "    all_data = []\n",
    "\n",
    "    for vid in video_ids:\n",
    "        try:\n",
    "            graph_sequence = fetch_graphs_from_neo4j(vid)\n",
    "            if graph_sequence:\n",
    "                all_data.append(graph_sequence)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading video {vid}: {e}\")\n",
    "\n",
    "    print(f\"✅ Loaded {len(all_data)} videos from DB\")\n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db5c46a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "class FreeThrowDataset(Dataset):\n",
    "    def __init__(self, matrix_data):\n",
    "        self.data = matrix_data\n",
    "        # The angle nodes are automatically mapped in our enhanced processing\n",
    "        # But we can still keep track of which landmarks are associated with angles\n",
    "        self.angle_node_landmarks = [\n",
    "            mp_pose.PoseLandmark.LEFT_ELBOW.value,\n",
    "            mp_pose.PoseLandmark.RIGHT_ELBOW.value,\n",
    "            mp_pose.PoseLandmark.LEFT_SHOULDER.value,\n",
    "            mp_pose.PoseLandmark.RIGHT_SHOULDER.value,\n",
    "            mp_pose.PoseLandmark.LEFT_KNEE.value,\n",
    "            mp_pose.PoseLandmark.RIGHT_KNEE.value,\n",
    "            mp_pose.PoseLandmark.LEFT_HIP.value,\n",
    "            mp_pose.PoseLandmark.RIGHT_HIP.value\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.data[idx]\n",
    "        data_sequence = []\n",
    "        \n",
    "        for timestep in sequence:\n",
    "            # Check if timestep is a dictionary (as expected)\n",
    "            if not isinstance(timestep, dict):\n",
    "                raise TypeError(f\"Expected dictionary, got {type(timestep)}. Value: {timestep}\")\n",
    "                \n",
    "            try:\n",
    "                edge_index = timestep['edge_index']\n",
    "                edge_attr = timestep['edge_attr']\n",
    "                label = timestep['label']\n",
    "                angle_features = timestep['angle_features']\n",
    "                \n",
    "                # Convert label to tensor if it's not already\n",
    "                if not isinstance(label, torch.Tensor):\n",
    "                    y = torch.tensor([label], dtype=torch.float)\n",
    "                else:\n",
    "                    y = label\n",
    "                \n",
    "                # Create graph data object with already remapped indices and features\n",
    "                data = Data(\n",
    "                    x=angle_features,  # Use angle features as node features\n",
    "                    edge_index=edge_index,\n",
    "                    edge_attr=edge_attr,\n",
    "                    y=y,\n",
    "                    num_nodes=angle_features.size(0)\n",
    "                )\n",
    "                \n",
    "                # Store additional information for reference\n",
    "                data.original_to_new_mapping = timestep['node_mapping']\n",
    "                data.new_to_original_mapping = timestep['reverse_mapping']\n",
    "                data.positional_features = timestep['node_features']  # Store original position features\n",
    "                data.time = timestep['time']\n",
    "                data.source_video = timestep['source_video']\n",
    "                \n",
    "                data_sequence.append(data)\n",
    "            except KeyError as e:\n",
    "                # Print detailed error info for debugging\n",
    "                print(f\"KeyError: {e} not found in timestep. Available keys: {list(timestep.keys())}\")\n",
    "                raise\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing timestep: {e}\")\n",
    "                raise\n",
    "            \n",
    "        return data_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for batching sequences.\n",
    "    Each batch item is a sequence of frames, and we want to\n",
    "    maintain these sequences.\n",
    "    \"\"\"\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5713fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(graphdata, train_ratio=0.7, random_seed=42):\n",
    "    \"\"\"\n",
    "    Split graph data while maintaining class distribution in both train and test sets.\n",
    "    \n",
    "    Args:\n",
    "        graphdata: List of graph sequences\n",
    "        train_ratio: Proportion of data for training (default: 0.7)\n",
    "        random_seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        train_matrix, test_matrix: Stratified splits of the data\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Group data by labels\n",
    "    label_to_sequences = defaultdict(list)\n",
    "    \n",
    "    for sequence in graphdata:\n",
    "        # Get label from first timestep (all timesteps in a sequence have same label)\n",
    "        if sequence and len(sequence) > 0:\n",
    "            label = sequence[0]['label']\n",
    "            label_to_sequences[label].append(sequence)\n",
    "    \n",
    "    # Print class distribution\n",
    "    print(\"Class distribution in dataset:\")\n",
    "    for label, sequences in label_to_sequences.items():\n",
    "        print(f\"  Class {label}: {len(sequences)} sequences ({len(sequences)/len(graphdata)*100:.1f}%)\")\n",
    "    \n",
    "    train_matrix = []\n",
    "    test_matrix = []\n",
    "    \n",
    "    # Split each class proportionally\n",
    "    for label, sequences in label_to_sequences.items():\n",
    "        # Shuffle sequences for this class\n",
    "        random.shuffle(sequences)\n",
    "        \n",
    "        # Calculate split point\n",
    "        n_train = int(len(sequences) * train_ratio)\n",
    "        \n",
    "        # Ensure at least one sample in test if possible\n",
    "        if len(sequences) > 1 and n_train == len(sequences):\n",
    "            n_train = len(sequences) - 1\n",
    "        \n",
    "        # Split the sequences\n",
    "        train_sequences = sequences[:n_train]\n",
    "        test_sequences = sequences[n_train:]\n",
    "        \n",
    "        train_matrix.extend(train_sequences)\n",
    "        test_matrix.extend(test_sequences)\n",
    "        \n",
    "        print(f\"  Class {label}: {len(train_sequences)} train, {len(test_sequences)} test\")\n",
    "    \n",
    "    # Final shuffle to mix classes\n",
    "    random.shuffle(train_matrix)\n",
    "    random.shuffle(test_matrix)\n",
    "    \n",
    "    print(f\"\\nFinal split: {len(train_matrix)} train, {len(test_matrix)} test\")\n",
    "    \n",
    "    # Verify class distribution is maintained\n",
    "    train_labels = [seq[0]['label'] for seq in train_matrix if seq]\n",
    "    test_labels = [seq[0]['label'] for seq in test_matrix if seq]\n",
    "    \n",
    "    print(f\"\\nTrain set class distribution:\")\n",
    "    train_counter = Counter(train_labels)\n",
    "    for label, count in sorted(train_counter.items()):\n",
    "        print(f\"  Class {label}: {count} ({count/len(train_labels)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nTest set class distribution:\")\n",
    "    test_counter = Counter(test_labels)\n",
    "    for label, count in sorted(test_counter.items()):\n",
    "        print(f\"  Class {label}: {count} ({count/len(test_labels)*100:.1f}%)\")\n",
    "    \n",
    "    return train_matrix, test_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e386d6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def stratified_balanced_split(graphdata, train_ratio=0.7, random_seed=42):\n",
    "    \"\"\"\n",
    "    Perform a balanced stratified split: equal number of sequences from each class\n",
    "    in both training and testing, limited by the smallest class count.\n",
    "    \n",
    "    Args:\n",
    "        graphdata: List of graph sequences\n",
    "        train_ratio: Proportion of data to use for training\n",
    "        random_seed: Seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        train_matrix, test_matrix: Balanced stratified splits\n",
    "    \"\"\"\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Group sequences by label\n",
    "    label_to_sequences = defaultdict(list)\n",
    "    for sequence in graphdata:\n",
    "        if sequence and len(sequence) > 0:\n",
    "            label = sequence[0]['label']\n",
    "            label_to_sequences[label].append(sequence)\n",
    "\n",
    "    # Display original class distribution\n",
    "    print(\"Original class distribution:\")\n",
    "    for label, seqs in label_to_sequences.items():\n",
    "        print(f\"  Class {label}: {len(seqs)} sequences\")\n",
    "\n",
    "    # Determine how many sequences to use per class (based on minority class)\n",
    "    min_class_count = min(len(seqs) for seqs in label_to_sequences.values())\n",
    "    print(f\"\\nBalancing to {min_class_count} sequences per class...\")\n",
    "\n",
    "    train_matrix = []\n",
    "    test_matrix = []\n",
    "\n",
    "    for label, sequences in label_to_sequences.items():\n",
    "        # Shuffle and trim to balanced size\n",
    "        random.shuffle(sequences)\n",
    "        balanced_sequences = sequences[:min_class_count]\n",
    "\n",
    "        # Split into train/test\n",
    "        n_train = int(train_ratio * min_class_count)\n",
    "        if min_class_count > 1 and n_train == min_class_count:\n",
    "            n_train = min_class_count - 1\n",
    "\n",
    "        train_sequences = balanced_sequences[:n_train]\n",
    "        test_sequences = balanced_sequences[n_train:]\n",
    "\n",
    "        train_matrix.extend(train_sequences)\n",
    "        test_matrix.extend(test_sequences)\n",
    "\n",
    "        print(f\"  Class {label}: {len(train_sequences)} train, {len(test_sequences)} test\")\n",
    "\n",
    "    # Shuffle final sets\n",
    "    random.shuffle(train_matrix)\n",
    "    random.shuffle(test_matrix)\n",
    "\n",
    "    print(f\"\\nFinal split: {len(train_matrix)} train, {len(test_matrix)} test\")\n",
    "\n",
    "    # Check class distributions\n",
    "    train_labels = [seq[0]['label'] for seq in train_matrix]\n",
    "    test_labels = [seq[0]['label'] for seq in test_matrix]\n",
    "\n",
    "    print(f\"\\nTrain set class distribution:\")\n",
    "    for label, count in sorted(Counter(train_labels).items()):\n",
    "        print(f\"  Class {label}: {count} ({count/len(train_labels)*100:.1f}%)\")\n",
    "\n",
    "    print(f\"\\nTest set class distribution:\")\n",
    "    for label, count in sorted(Counter(test_labels).items()):\n",
    "        print(f\"  Class {label}: {count} ({count/len(test_labels)*100:.1f}%)\")\n",
    "\n",
    "    return train_matrix, test_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14cf32de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stratified_datasets(graphdata, train_ratio=0.7, random_seed=42):\n",
    "    \"\"\"\n",
    "    Create stratified train/test datasets with proper class balance.\n",
    "    \"\"\"\n",
    "    train_matrix, test_matrix = stratified_balanced_split(graphdata, train_ratio, random_seed)\n",
    "    \n",
    "    train_dataset = FreeThrowDataset(train_matrix)\n",
    "    test_dataset = FreeThrowDataset(test_matrix)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    return train_dataset, test_dataset, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2ae6874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_LSTM(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, lstm_hidden, num_classes):\n",
    "        super().__init__()\n",
    "        self.gcn = GCNConv(in_channels, hidden_channels)\n",
    "        self.lstm = nn.LSTM(hidden_channels, lstm_hidden, batch_first=True)\n",
    "        self.classifier = nn.Linear(lstm_hidden, num_classes)\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        gcn_outputs = []\n",
    "        for data in sequence:\n",
    "            x = self.gcn(data.x, data.edge_index)\n",
    "            x = torch.relu(x)\n",
    "            pooled = x.mean(dim=0)  # Global mean pooling\n",
    "            gcn_outputs.append(pooled)\n",
    "\n",
    "        gcn_outputs = torch.stack(gcn_outputs).unsqueeze(0)  # [1, T, F]\n",
    "        lstm_out, _ = self.lstm(gcn_outputs)\n",
    "        out = self.classifier(lstm_out[:, -1, :])  # Use last time step\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb3f0b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_LSTM_SpatialAttention(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, lstm_hidden, num_classes, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.gat = GATConv(in_channels, hidden_channels, heads=num_heads, concat=False, dropout=0.1)\n",
    "        self.lstm = nn.LSTM(hidden_channels, lstm_hidden, batch_first=True)\n",
    "        self.classifier = nn.Linear(lstm_hidden, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        gcn_outputs = []\n",
    "        for data in sequence:\n",
    "            x = self.gat(data.x, data.edge_index)\n",
    "            x = torch.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            pooled = x.mean(dim=0)  # Global mean pooling\n",
    "            gcn_outputs.append(pooled)\n",
    "\n",
    "        gcn_outputs = torch.stack(gcn_outputs).unsqueeze(0)  # [1, T, F]\n",
    "        lstm_out, _ = self.lstm(gcn_outputs)\n",
    "        out = self.classifier(lstm_out[:, -1, :])  # Use last time step\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0b34a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, lstm_output):\n",
    "        # Compute attention weights for each time step\n",
    "        attn_weights = self.attention(lstm_output)  # [batch_size, seq_len, 1]\n",
    "        attn_weights = F.softmax(attn_weights, dim=1)  # Normalize over time\n",
    "        \n",
    "        # Weighted sum over time dimension\n",
    "        attended_output = torch.sum(lstm_output * attn_weights, dim=1)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        return attended_output, attn_weights.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f769291",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(input_dim // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Compute attention weights for each node\n",
    "        attn_weights = self.attention(x)  # [num_nodes, 1]\n",
    "        attn_weights = F.softmax(attn_weights, dim=0)  # Normalize\n",
    "        \n",
    "        # Weighted sum of node features\n",
    "        pooled = torch.sum(x * attn_weights, dim=0)  # [input_dim]\n",
    "        return pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffb4236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_LSTM_TemporalAttention(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, lstm_hidden, num_classes):\n",
    "        super().__init__()\n",
    "        self.gcn = GCNConv(in_channels, hidden_channels)\n",
    "        self.lstm = nn.LSTM(hidden_channels, lstm_hidden, batch_first=True)\n",
    "        self.temporal_attention = TemporalAttention(lstm_hidden)\n",
    "        self.classifier = nn.Linear(lstm_hidden, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        gcn_outputs = []\n",
    "        for data in sequence:\n",
    "            x = self.gcn(data.x, data.edge_index)\n",
    "            x = torch.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            pooled = x.mean(dim=0)\n",
    "            gcn_outputs.append(pooled)\n",
    "\n",
    "        gcn_outputs = torch.stack(gcn_outputs).unsqueeze(0)  # [1, T, F]\n",
    "        lstm_out, _ = self.lstm(gcn_outputs)\n",
    "        \n",
    "        # Apply temporal attention instead of using just last time step\n",
    "        attended_output, attention_weights = self.temporal_attention(lstm_out)\n",
    "        out = self.classifier(attended_output)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "546fd2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_LSTM_FullAttention(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, lstm_hidden, num_classes, num_heads=4):\n",
    "        super().__init__()\n",
    "        # Spatial attention with GAT\n",
    "        self.gat = GATConv(in_channels, hidden_channels, heads=num_heads, concat=False, dropout=0.1)\n",
    "        \n",
    "        # Attention pooling\n",
    "        self.attention_pool = AttentionPooling(hidden_channels)\n",
    "        \n",
    "        # LSTM with temporal attention\n",
    "        self.lstm = nn.LSTM(hidden_channels, lstm_hidden, batch_first=True)\n",
    "        self.temporal_attention = TemporalAttention(lstm_hidden)\n",
    "        \n",
    "        # Classification\n",
    "        self.classifier = nn.Linear(lstm_hidden, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_channels)\n",
    "        self.layer_norm2 = nn.LayerNorm(lstm_hidden)\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        gcn_outputs = []\n",
    "        \n",
    "        for data in sequence:\n",
    "            # Spatial attention with GAT\n",
    "            x = self.gat(data.x, data.edge_index)\n",
    "            x = self.layer_norm1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "            # Attention-based pooling\n",
    "            pooled = self.attention_pool(x)\n",
    "            gcn_outputs.append(pooled)\n",
    "\n",
    "        gcn_outputs = torch.stack(gcn_outputs).unsqueeze(0)  # [1, T, F]\n",
    "        \n",
    "        # LSTM processing\n",
    "        lstm_out, _ = self.lstm(gcn_outputs)\n",
    "        lstm_out = self.layer_norm2(lstm_out)\n",
    "        \n",
    "        # Temporal attention\n",
    "        attended_output, temporal_attention_weights = self.temporal_attention(lstm_out)\n",
    "        \n",
    "        # Final classification\n",
    "        out = self.classifier(attended_output)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f293e3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 109 videos from DB\n"
     ]
    }
   ],
   "source": [
    "graphdata=load_graph_sequences_from_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cf14209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution:\n",
      "  Class 1: 79 sequences\n",
      "  Class 0: 30 sequences\n",
      "\n",
      "Balancing to 30 sequences per class...\n",
      "  Class 1: 21 train, 9 test\n",
      "  Class 0: 21 train, 9 test\n",
      "\n",
      "Final split: 42 train, 18 test\n",
      "\n",
      "Train set class distribution:\n",
      "  Class 0: 21 (50.0%)\n",
      "  Class 1: 21 (50.0%)\n",
      "\n",
      "Test set class distribution:\n",
      "  Class 0: 9 (50.0%)\n",
      "  Class 1: 9 (50.0%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset,test_dataset,train_loader,test_loader=create_stratified_datasets(graphdata,0.7,42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ef8ff0",
   "metadata": {},
   "source": [
    "# TGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d86df277",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN_LSTM(in_channels=1, hidden_channels=32, lstm_hidden=16, num_classes=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e56790f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 30.5084\n",
      "Epoch 2 - Loss: 29.4286\n",
      "Epoch 3 - Loss: 29.2687\n",
      "Epoch 4 - Loss: 29.2798\n",
      "Epoch 5 - Loss: 29.5883\n",
      "Epoch 6 - Loss: 29.2298\n",
      "Epoch 7 - Loss: 29.3196\n",
      "Epoch 8 - Loss: 29.2072\n",
      "Epoch 9 - Loss: 29.8774\n",
      "Epoch 10 - Loss: 29.1038\n",
      "Epoch 11 - Loss: 29.1798\n",
      "Epoch 12 - Loss: 29.2795\n",
      "Epoch 13 - Loss: 29.1435\n",
      "Epoch 14 - Loss: 29.1717\n",
      "Epoch 15 - Loss: 29.1438\n",
      "Epoch 16 - Loss: 29.1392\n",
      "Epoch 17 - Loss: 29.1835\n",
      "Epoch 18 - Loss: 29.0743\n",
      "Epoch 19 - Loss: 29.1531\n",
      "Epoch 20 - Loss: 29.1389\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        sequence = batch[0]  # batch size = 1\n",
    "        target = sequence[0].y\n",
    "        output = model(sequence)\n",
    "        loss = loss_fn(output.view(-1), target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "329d4b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (manual): 9/18 = 50.00%\n",
      "Accuracy (sklearn): 0.5000\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.6667\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         9\n",
      "           1       0.50      1.00      0.67         9\n",
      "\n",
      "    accuracy                           0.50        18\n",
      "   macro avg       0.25      0.50      0.33        18\n",
      "weighted avg       0.25      0.50      0.33        18\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayman\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ayman\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ayman\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        sequence = batch[0]\n",
    "        target = int(sequence[0].y.item())\n",
    "        output = model(sequence)\n",
    "        prediction = (torch.sigmoid(output) > 0.5).int().item()\n",
    "        \n",
    "        correct += int(prediction == target)\n",
    "        total += 1\n",
    "        \n",
    "        all_predictions.append(prediction)\n",
    "        all_targets.append(target)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_manual = correct / total\n",
    "accuracy = accuracy_score(all_targets, all_predictions)\n",
    "recall = recall_score(all_targets, all_predictions, average='binary')  # for binary classification\n",
    "f1 = f1_score(all_targets, all_predictions, average='binary')  # for binary classification\n",
    "\n",
    "# Print results\n",
    "print(f\"Test Accuracy (manual): {correct}/{total} = {accuracy_manual:.2%}\")\n",
    "print(f\"Accuracy (sklearn): {accuracy:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Optional: Print detailed classification report\n",
    "print('\\nDetailed Classification Report:')\n",
    "print(classification_report(all_targets, all_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d63105",
   "metadata": {},
   "source": [
    "# GNN Spatial Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "680d1341",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN_LSTM_SpatialAttention(in_channels=1, hidden_channels=32, lstm_hidden=16, num_classes=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4de6608a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 29.5730\n",
      "Epoch 2 - Loss: 29.2203\n",
      "Epoch 3 - Loss: 29.3043\n",
      "Epoch 4 - Loss: 29.2643\n",
      "Epoch 5 - Loss: 29.3031\n",
      "Epoch 6 - Loss: 29.2665\n",
      "Epoch 7 - Loss: 29.4636\n",
      "Epoch 8 - Loss: 29.3315\n",
      "Epoch 9 - Loss: 29.2497\n",
      "Epoch 10 - Loss: 29.1001\n",
      "Epoch 11 - Loss: 29.3126\n",
      "Epoch 12 - Loss: 29.1332\n",
      "Epoch 13 - Loss: 29.3020\n",
      "Epoch 14 - Loss: 29.1122\n",
      "Epoch 15 - Loss: 29.1615\n",
      "Epoch 16 - Loss: 29.1709\n",
      "Epoch 17 - Loss: 29.1272\n",
      "Epoch 18 - Loss: 29.0364\n",
      "Epoch 19 - Loss: 29.1871\n",
      "Epoch 20 - Loss: 29.2570\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        sequence = batch[0]  # batch size = 1\n",
    "        target = sequence[0].y\n",
    "        output = model(sequence)\n",
    "        loss = loss_fn(output.view(-1), target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddf46a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (manual): 11/18 = 61.11%\n",
      "Accuracy (sklearn): 0.6111\n",
      "Recall: 0.7778\n",
      "F1 Score: 0.6667\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.44      0.53         9\n",
      "           1       0.58      0.78      0.67         9\n",
      "\n",
      "    accuracy                           0.61        18\n",
      "   macro avg       0.62      0.61      0.60        18\n",
      "weighted avg       0.62      0.61      0.60        18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        sequence = batch[0]\n",
    "        target = int(sequence[0].y.item())\n",
    "        output = model(sequence)\n",
    "        prediction = (torch.sigmoid(output) > 0.5).int().item()\n",
    "        \n",
    "        correct += int(prediction == target)\n",
    "        total += 1\n",
    "        \n",
    "        all_predictions.append(prediction)\n",
    "        all_targets.append(target)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_manual = correct / total\n",
    "accuracy = accuracy_score(all_targets, all_predictions)\n",
    "recall = recall_score(all_targets, all_predictions, average='binary')  # for binary classification\n",
    "f1 = f1_score(all_targets, all_predictions, average='binary')  # for binary classification\n",
    "\n",
    "# Print results\n",
    "print(f\"Test Accuracy (manual): {correct}/{total} = {accuracy_manual:.2%}\")\n",
    "print(f\"Accuracy (sklearn): {accuracy:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Optional: Print detailed classification report\n",
    "print('\\nDetailed Classification Report:')\n",
    "print(classification_report(all_targets, all_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a2b275",
   "metadata": {},
   "source": [
    "# GNN Temporal Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15a9e163",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN_LSTM_TemporalAttention(in_channels=1, hidden_channels=32, lstm_hidden=16, num_classes=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2603c67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 29.8273\n",
      "Epoch 2 - Loss: 29.4909\n",
      "Epoch 3 - Loss: 29.3906\n",
      "Epoch 4 - Loss: 29.2631\n",
      "Epoch 5 - Loss: 29.1875\n",
      "Epoch 6 - Loss: 29.3073\n",
      "Epoch 7 - Loss: 29.2141\n",
      "Epoch 8 - Loss: 29.1474\n",
      "Epoch 9 - Loss: 29.1572\n",
      "Epoch 10 - Loss: 29.1387\n",
      "Epoch 11 - Loss: 29.1356\n",
      "Epoch 12 - Loss: 29.1852\n",
      "Epoch 13 - Loss: 29.1417\n",
      "Epoch 14 - Loss: 29.1125\n",
      "Epoch 15 - Loss: 29.1085\n",
      "Epoch 16 - Loss: 29.0941\n",
      "Epoch 17 - Loss: 29.1915\n",
      "Epoch 18 - Loss: 29.0976\n",
      "Epoch 19 - Loss: 29.0824\n",
      "Epoch 20 - Loss: 29.0970\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        sequence = batch[0]  # batch size = 1\n",
    "        target = sequence[0].y\n",
    "        output = model(sequence)\n",
    "        loss = loss_fn(output.view(-1), target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65259dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (manual): 9/18 = 50.00%\n",
      "Accuracy (sklearn): 0.5000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         9\n",
      "           1       0.00      0.00      0.00         9\n",
      "\n",
      "    accuracy                           0.50        18\n",
      "   macro avg       0.25      0.50      0.33        18\n",
      "weighted avg       0.25      0.50      0.33        18\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayman\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ayman\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ayman\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        sequence = batch[0]\n",
    "        target = int(sequence[0].y.item())\n",
    "        output = model(sequence)\n",
    "        prediction = (torch.sigmoid(output) > 0.5).int().item()\n",
    "        \n",
    "        correct += int(prediction == target)\n",
    "        total += 1\n",
    "        \n",
    "        all_predictions.append(prediction)\n",
    "        all_targets.append(target)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_manual = correct / total\n",
    "accuracy = accuracy_score(all_targets, all_predictions)\n",
    "recall = recall_score(all_targets, all_predictions, average='binary')  # for binary classification\n",
    "f1 = f1_score(all_targets, all_predictions, average='binary')  # for binary classification\n",
    "\n",
    "# Print results\n",
    "print(f\"Test Accuracy (manual): {correct}/{total} = {accuracy_manual:.2%}\")\n",
    "print(f\"Accuracy (sklearn): {accuracy:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Optional: Print detailed classification report\n",
    "print('\\nDetailed Classification Report:')\n",
    "print(classification_report(all_targets, all_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bbe692",
   "metadata": {},
   "source": [
    "# GNN Temporal + Spatial Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2c4d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN_LSTM_FullAttention(in_channels=1, hidden_channels=32, lstm_hidden=16, num_classes=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "711921cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 31.1536\n",
      "Epoch 2 - Loss: 29.6593\n",
      "Epoch 3 - Loss: 29.4238\n",
      "Epoch 4 - Loss: 29.6290\n",
      "Epoch 5 - Loss: 29.0253\n",
      "Epoch 6 - Loss: 28.9259\n",
      "Epoch 7 - Loss: 29.2086\n",
      "Epoch 8 - Loss: 28.9513\n",
      "Epoch 9 - Loss: 28.1176\n",
      "Epoch 10 - Loss: 28.4098\n",
      "Epoch 11 - Loss: 28.3028\n",
      "Epoch 12 - Loss: 28.1333\n",
      "Epoch 13 - Loss: 27.9079\n",
      "Epoch 14 - Loss: 27.2741\n",
      "Epoch 15 - Loss: 26.6381\n",
      "Epoch 16 - Loss: 26.9597\n",
      "Epoch 17 - Loss: 28.5227\n",
      "Epoch 18 - Loss: 25.8668\n",
      "Epoch 19 - Loss: 26.7569\n",
      "Epoch 20 - Loss: 26.2356\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        sequence = batch[0]  # batch size = 1\n",
    "        target = sequence[0].y\n",
    "        output = model(sequence)\n",
    "        loss = loss_fn(output.view(-1), target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "567cd5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy (manual): 8/18 = 44.44%\n",
      "Accuracy (sklearn): 0.4444\n",
      "Recall: 0.2222\n",
      "F1 Score: 0.2857\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.67      0.55         9\n",
      "           1       0.40      0.22      0.29         9\n",
      "\n",
      "    accuracy                           0.44        18\n",
      "   macro avg       0.43      0.44      0.42        18\n",
      "weighted avg       0.43      0.44      0.42        18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        sequence = batch[0]\n",
    "        target = int(sequence[0].y.item())\n",
    "        output = model(sequence)\n",
    "        prediction = (torch.sigmoid(output) > 0.5).int().item()\n",
    "        \n",
    "        correct += int(prediction == target)\n",
    "        total += 1\n",
    "        \n",
    "        all_predictions.append(prediction)\n",
    "        all_targets.append(target)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_manual = correct / total\n",
    "accuracy = accuracy_score(all_targets, all_predictions)\n",
    "recall = recall_score(all_targets, all_predictions, average='binary')  # for binary classification\n",
    "f1 = f1_score(all_targets, all_predictions, average='binary')  # for binary classification\n",
    "\n",
    "# Print results\n",
    "print(f\"Test Accuracy (manual): {correct}/{total} = {accuracy_manual:.2%}\")\n",
    "print(f\"Accuracy (sklearn): {accuracy:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Optional: Print detailed classification report\n",
    "print('\\nDetailed Classification Report:')\n",
    "print(classification_report(all_targets, all_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
