{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8665539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv,GATConv\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import mediapipe as mp\n",
    "from pymongo import MongoClient\n",
    "from neo4j import GraphDatabase\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4914e551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=2,  \n",
    "    smooth_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e7e2a04",
   "metadata": {},
   "outputs": [],
   "source": [
    " #MongoDB connection\n",
    "mongo_client = MongoClient(\"mongodb://admin:password@localhost:27017/\")\n",
    "mongo_db = mongo_client[\"SportsAnalysis\"]\n",
    "labels_collection = mongo_db[\"metadata\"]\n",
    "\n",
    "# Neo4j connection\n",
    "neo4j_driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc1a7677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_video_ids():\n",
    "        return [doc['video_id'] for doc in labels_collection.find({}, {'video_id': 1})]\n",
    "\n",
    "def fetch_label(video_id):\n",
    "        doc = labels_collection.find_one({'video_id': video_id})\n",
    "        return doc['label'] if doc else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d6c45e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_graphs_from_neo4j(video_id):\n",
    "        with neo4j_driver.session() as session:\n",
    "            # Get all unique time steps\n",
    "            result = session.run(\"\"\"\n",
    "                MATCH (n:PoseNode {video_id: $video_id})\n",
    "                RETURN DISTINCT n.time_index AS timestep\n",
    "                ORDER BY timestep ASC\n",
    "            \"\"\", video_id=video_id)\n",
    "            time_steps = [record[\"timestep\"] for record in result]\n",
    "\n",
    "            graphs = []\n",
    "            for t in time_steps:\n",
    "                # Fetch nodes\n",
    "                node_query = session.run(\"\"\"\n",
    "                    MATCH (n:PoseNode {video_id: $video_id, time_index: $t})\n",
    "                    RETURN n.node_index AS idx, n.angle AS angle, n.time AS time\n",
    "                    ORDER BY idx\n",
    "                \"\"\", video_id=video_id, t=t)\n",
    "\n",
    "                node_data = []\n",
    "                time_value = 0\n",
    "                for record in node_query:\n",
    "                    node_data.append(float(record[\"angle\"]))\n",
    "                    time_value = float(record[\"time\"])\n",
    "\n",
    "                x = torch.tensor(node_data, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "                # Fetch edges\n",
    "                edge_query = session.run(\"\"\"\n",
    "                    MATCH (a:PoseNode {video_id: $video_id, time_index: $t})-[r:CONNECTED_TO]->(b:PoseNode)\n",
    "                    RETURN a.node_index AS src, b.node_index AS dst, r.weight AS weight\n",
    "                \"\"\", video_id=video_id, t=t)\n",
    "\n",
    "                edge_index = []\n",
    "                edge_attr = []\n",
    "                for record in edge_query:\n",
    "                    edge_index.append([int(record[\"src\"]), int(record[\"dst\"])])\n",
    "                    edge_attr.append([float(record[\"weight\"])])\n",
    "\n",
    "                edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "                edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "\n",
    "                graphs.append({\n",
    "                    \"edge_index\": edge_index,\n",
    "                    \"edge_attr\": edge_attr,\n",
    "                    \"angle_features\": x,\n",
    "                    \"time\": time_value,\n",
    "                    \"source_video\": video_id,\n",
    "                    \"label\": fetch_label(video_id),\n",
    "                    \"node_mapping\": {},  # Optional: mapping if you have remapped indices\n",
    "                    \"reverse_mapping\": {},\n",
    "                    \"node_features\": x.clone()\n",
    "                })\n",
    "            return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffa79a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph_sequences_from_db():\n",
    "    video_ids = fetch_all_video_ids()\n",
    "    all_data = []\n",
    "\n",
    "    for vid in video_ids:\n",
    "        try:\n",
    "            graph_sequence = fetch_graphs_from_neo4j(vid)\n",
    "            if graph_sequence:\n",
    "                all_data.append(graph_sequence)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading video {vid}: {e}\")\n",
    "\n",
    "    print(f\"✅ Loaded {len(all_data)} videos from DB\")\n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db5c46a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "class FreeThrowDataset(Dataset):\n",
    "    def __init__(self, matrix_data):\n",
    "        self.data = matrix_data\n",
    "        # The angle nodes are automatically mapped in our enhanced processing\n",
    "        # But we can still keep track of which landmarks are associated with angles\n",
    "        self.angle_node_landmarks = [\n",
    "            mp_pose.PoseLandmark.LEFT_ELBOW.value,\n",
    "            mp_pose.PoseLandmark.RIGHT_ELBOW.value,\n",
    "            mp_pose.PoseLandmark.LEFT_SHOULDER.value,\n",
    "            mp_pose.PoseLandmark.RIGHT_SHOULDER.value,\n",
    "            mp_pose.PoseLandmark.LEFT_KNEE.value,\n",
    "            mp_pose.PoseLandmark.RIGHT_KNEE.value,\n",
    "            mp_pose.PoseLandmark.LEFT_HIP.value,\n",
    "            mp_pose.PoseLandmark.RIGHT_HIP.value\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.data[idx]\n",
    "        data_sequence = []\n",
    "        \n",
    "        for timestep in sequence:\n",
    "            # Check if timestep is a dictionary (as expected)\n",
    "            if not isinstance(timestep, dict):\n",
    "                raise TypeError(f\"Expected dictionary, got {type(timestep)}. Value: {timestep}\")\n",
    "                \n",
    "            try:\n",
    "                edge_index = timestep['edge_index']\n",
    "                edge_attr = timestep['edge_attr']\n",
    "                label = timestep['label']\n",
    "                angle_features = timestep['angle_features']\n",
    "                \n",
    "                # Convert label to tensor if it's not already\n",
    "                if not isinstance(label, torch.Tensor):\n",
    "                    y = torch.tensor([label], dtype=torch.float)\n",
    "                else:\n",
    "                    y = label\n",
    "                \n",
    "                # Create graph data object with already remapped indices and features\n",
    "                data = Data(\n",
    "                    x=angle_features,  # Use angle features as node features\n",
    "                    edge_index=edge_index,\n",
    "                    edge_attr=edge_attr,\n",
    "                    y=y,\n",
    "                    num_nodes=angle_features.size(0)\n",
    "                )\n",
    "                \n",
    "                # Store additional information for reference\n",
    "                data.original_to_new_mapping = timestep['node_mapping']\n",
    "                data.new_to_original_mapping = timestep['reverse_mapping']\n",
    "                data.positional_features = timestep['node_features']  # Store original position features\n",
    "                data.time = timestep['time']\n",
    "                data.source_video = timestep['source_video']\n",
    "                \n",
    "                data_sequence.append(data)\n",
    "            except KeyError as e:\n",
    "                # Print detailed error info for debugging\n",
    "                print(f\"KeyError: {e} not found in timestep. Available keys: {list(timestep.keys())}\")\n",
    "                raise\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing timestep: {e}\")\n",
    "                raise\n",
    "            \n",
    "        return data_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for batching sequences.\n",
    "    Each batch item is a sequence of frames, and we want to\n",
    "    maintain these sequences.\n",
    "    \"\"\"\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2ae6874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_LSTM(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, lstm_hidden, num_classes):\n",
    "        super().__init__()\n",
    "        self.gcn = GCNConv(in_channels, hidden_channels)\n",
    "        self.lstm = nn.LSTM(hidden_channels, lstm_hidden, batch_first=True)\n",
    "        self.classifier = nn.Linear(lstm_hidden, num_classes)\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        gcn_outputs = []\n",
    "        for data in sequence:\n",
    "            x = self.gcn(data.x, data.edge_index)\n",
    "            x = torch.relu(x)\n",
    "            pooled = x.mean(dim=0)  # Global mean pooling\n",
    "            gcn_outputs.append(pooled)\n",
    "\n",
    "        gcn_outputs = torch.stack(gcn_outputs).unsqueeze(0)  # [1, T, F]\n",
    "        lstm_out, _ = self.lstm(gcn_outputs)\n",
    "        out = self.classifier(lstm_out[:, -1, :])  # Use last time step\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb3f0b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_LSTM_SpatialAttention(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, lstm_hidden, num_classes, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.gat = GATConv(in_channels, hidden_channels, heads=num_heads, concat=False, dropout=0.1)\n",
    "        self.lstm = nn.LSTM(hidden_channels, lstm_hidden, batch_first=True)\n",
    "        self.classifier = nn.Linear(lstm_hidden, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        gcn_outputs = []\n",
    "        for data in sequence:\n",
    "            x = self.gat(data.x, data.edge_index)\n",
    "            x = torch.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            pooled = x.mean(dim=0)  # Global mean pooling\n",
    "            gcn_outputs.append(pooled)\n",
    "\n",
    "        gcn_outputs = torch.stack(gcn_outputs).unsqueeze(0)  # [1, T, F]\n",
    "        lstm_out, _ = self.lstm(gcn_outputs)\n",
    "        out = self.classifier(lstm_out[:, -1, :])  # Use last time step\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0b34a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, lstm_output):\n",
    "        # Compute attention weights for each time step\n",
    "        attn_weights = self.attention(lstm_output)  # [batch_size, seq_len, 1]\n",
    "        attn_weights = F.softmax(attn_weights, dim=1)  # Normalize over time\n",
    "        \n",
    "        # Weighted sum over time dimension\n",
    "        attended_output = torch.sum(lstm_output * attn_weights, dim=1)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        return attended_output, attn_weights.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f769291",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(input_dim // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Compute attention weights for each node\n",
    "        attn_weights = self.attention(x)  # [num_nodes, 1]\n",
    "        attn_weights = F.softmax(attn_weights, dim=0)  # Normalize\n",
    "        \n",
    "        # Weighted sum of node features\n",
    "        pooled = torch.sum(x * attn_weights, dim=0)  # [input_dim]\n",
    "        return pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ffb4236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_LSTM_TemporalAttention(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, lstm_hidden, num_classes):\n",
    "        super().__init__()\n",
    "        self.gcn = GCNConv(in_channels, hidden_channels)\n",
    "        self.lstm = nn.LSTM(hidden_channels, lstm_hidden, batch_first=True)\n",
    "        self.temporal_attention = TemporalAttention(lstm_hidden)\n",
    "        self.classifier = nn.Linear(lstm_hidden, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        gcn_outputs = []\n",
    "        for data in sequence:\n",
    "            x = self.gcn(data.x, data.edge_index)\n",
    "            x = torch.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            pooled = x.mean(dim=0)\n",
    "            gcn_outputs.append(pooled)\n",
    "\n",
    "        gcn_outputs = torch.stack(gcn_outputs).unsqueeze(0)  # [1, T, F]\n",
    "        lstm_out, _ = self.lstm(gcn_outputs)\n",
    "        \n",
    "        # Apply temporal attention instead of using just last time step\n",
    "        attended_output, attention_weights = self.temporal_attention(lstm_out)\n",
    "        out = self.classifier(attended_output)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "546fd2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_LSTM_FullAttention(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, lstm_hidden, num_classes, num_heads=4):\n",
    "        super().__init__()\n",
    "        # Spatial attention with GAT\n",
    "        self.gat = GATConv(in_channels, hidden_channels, heads=num_heads, concat=False, dropout=0.1)\n",
    "        \n",
    "        # Attention pooling\n",
    "        self.attention_pool = AttentionPooling(hidden_channels)\n",
    "        \n",
    "        # LSTM with temporal attention\n",
    "        self.lstm = nn.LSTM(hidden_channels, lstm_hidden, batch_first=True)\n",
    "        self.temporal_attention = TemporalAttention(lstm_hidden)\n",
    "        \n",
    "        # Classification\n",
    "        self.classifier = nn.Linear(lstm_hidden, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_channels)\n",
    "        self.layer_norm2 = nn.LayerNorm(lstm_hidden)\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        gcn_outputs = []\n",
    "        \n",
    "        for data in sequence:\n",
    "            # Spatial attention with GAT\n",
    "            x = self.gat(data.x, data.edge_index)\n",
    "            x = self.layer_norm1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "            # Attention-based pooling\n",
    "            pooled = self.attention_pool(x)\n",
    "            gcn_outputs.append(pooled)\n",
    "\n",
    "        gcn_outputs = torch.stack(gcn_outputs).unsqueeze(0)  # [1, T, F]\n",
    "        \n",
    "        # LSTM processing\n",
    "        lstm_out, _ = self.lstm(gcn_outputs)\n",
    "        lstm_out = self.layer_norm2(lstm_out)\n",
    "        \n",
    "        # Temporal attention\n",
    "        attended_output, temporal_attention_weights = self.temporal_attention(lstm_out)\n",
    "        \n",
    "        # Final classification\n",
    "        out = self.classifier(attended_output)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f293e3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 70 videos from DB\n"
     ]
    }
   ],
   "source": [
    "graphdata=load_graph_sequences_from_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d4b0367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "print(len(graphdata))\n",
    "print(len(graphdata[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cf14209",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(graphdata)\n",
    "split = int(0.7 * len(graphdata))\n",
    "train_matrix = graphdata[:split]\n",
    "test_matrix = graphdata[split:]\n",
    "\n",
    "train_dataset = FreeThrowDataset(train_matrix)\n",
    "test_dataset = FreeThrowDataset(test_matrix)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d86df277",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN_LSTM(in_channels=1, hidden_channels=32, lstm_hidden=16, num_classes=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e56790f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 28.4931\n",
      "Epoch 2 - Loss: 26.7599\n",
      "Epoch 3 - Loss: 26.2548\n",
      "Epoch 4 - Loss: 26.0428\n",
      "Epoch 5 - Loss: 26.1971\n",
      "Epoch 6 - Loss: 25.8163\n",
      "Epoch 7 - Loss: 25.7570\n",
      "Epoch 8 - Loss: 26.0463\n",
      "Epoch 9 - Loss: 26.7871\n",
      "Epoch 10 - Loss: 25.9737\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        sequence = batch[0]  # batch size = 1\n",
    "        target = sequence[0].y\n",
    "        output = model(sequence)\n",
    "        loss = loss_fn(output.view(-1), target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "329d4b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 16/21 = 76.19%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        sequence = batch[0]\n",
    "        target = int(sequence[0].y.item())\n",
    "        output = model(sequence)\n",
    "        prediction = (torch.sigmoid(output) > 0.5).int().item()\n",
    "        correct += int(prediction == target)\n",
    "        total += 1\n",
    "\n",
    "print(f\"Test Accuracy: {correct}/{total} = {correct / total:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "680d1341",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN_LSTM_SpatialAttention(in_channels=1, hidden_channels=32, lstm_hidden=16, num_classes=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4de6608a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 28.7019\n",
      "Epoch 2 - Loss: 26.7766\n",
      "Epoch 3 - Loss: 26.2127\n",
      "Epoch 4 - Loss: 26.2357\n",
      "Epoch 5 - Loss: 25.9391\n",
      "Epoch 6 - Loss: 26.2687\n",
      "Epoch 7 - Loss: 25.8865\n",
      "Epoch 8 - Loss: 26.0219\n",
      "Epoch 9 - Loss: 26.6884\n",
      "Epoch 10 - Loss: 25.8538\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        sequence = batch[0]  # batch size = 1\n",
    "        target = sequence[0].y\n",
    "        output = model(sequence)\n",
    "        loss = loss_fn(output.view(-1), target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddf46a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 16/21 = 76.19%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        sequence = batch[0]\n",
    "        target = int(sequence[0].y.item())\n",
    "        output = model(sequence)\n",
    "        prediction = (torch.sigmoid(output) > 0.5).int().item()\n",
    "        correct += int(prediction == target)\n",
    "        total += 1\n",
    "\n",
    "print(f\"Test Accuracy: {correct}/{total} = {correct / total:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15a9e163",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN_LSTM_TemporalAttention(in_channels=1, hidden_channels=32, lstm_hidden=16, num_classes=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2603c67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 30.3347\n",
      "Epoch 2 - Loss: 27.3175\n",
      "Epoch 3 - Loss: 26.5450\n",
      "Epoch 4 - Loss: 26.2978\n",
      "Epoch 5 - Loss: 26.1299\n",
      "Epoch 6 - Loss: 26.0541\n",
      "Epoch 7 - Loss: 26.1237\n",
      "Epoch 8 - Loss: 26.0217\n",
      "Epoch 9 - Loss: 26.1824\n",
      "Epoch 10 - Loss: 27.1371\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        sequence = batch[0]  # batch size = 1\n",
    "        target = sequence[0].y\n",
    "        output = model(sequence)\n",
    "        loss = loss_fn(output.view(-1), target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "65259dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 16/21 = 76.19%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        sequence = batch[0]\n",
    "        target = int(sequence[0].y.item())\n",
    "        output = model(sequence)\n",
    "        prediction = (torch.sigmoid(output) > 0.5).int().item()\n",
    "        correct += int(prediction == target)\n",
    "        total += 1\n",
    "\n",
    "print(f\"Test Accuracy: {correct}/{total} = {correct / total:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2c4d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN_LSTM_FullAttention(in_channels=1, hidden_channels=32, lstm_hidden=16, num_classes=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "711921cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 28.5177\n",
      "Epoch 2 - Loss: 26.7081\n",
      "Epoch 3 - Loss: 26.6307\n",
      "Epoch 4 - Loss: 26.4344\n",
      "Epoch 5 - Loss: 26.3164\n",
      "Epoch 6 - Loss: 26.1486\n",
      "Epoch 7 - Loss: 25.9159\n",
      "Epoch 8 - Loss: 26.4387\n",
      "Epoch 9 - Loss: 25.8155\n",
      "Epoch 10 - Loss: 25.7764\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        sequence = batch[0]  # batch size = 1\n",
    "        target = sequence[0].y\n",
    "        output = model(sequence)\n",
    "        loss = loss_fn(output.view(-1), target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "567cd5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 16/21 = 76.19%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        sequence = batch[0]\n",
    "        target = int(sequence[0].y.item())\n",
    "        output = model(sequence)\n",
    "        prediction = (torch.sigmoid(output) > 0.5).int().item()\n",
    "        correct += int(prediction == target)\n",
    "        total += 1\n",
    "\n",
    "print(f\"Test Accuracy: {correct}/{total} = {correct / total:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
