{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8665539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import random\n",
    "import mediapipe as mp\n",
    "from pymongo import MongoClient\n",
    "from neo4j import GraphDatabase\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4914e551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=2,  \n",
    "    smooth_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e7e2a04",
   "metadata": {},
   "outputs": [],
   "source": [
    " #MongoDB connection\n",
    "mongo_client = MongoClient(\"mongodb://admin:password@localhost:27017/\")\n",
    "mongo_db = mongo_client[\"SportsAnalysis\"]\n",
    "labels_collection = mongo_db[\"metadata\"]\n",
    "\n",
    "# Neo4j connection\n",
    "neo4j_driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc1a7677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_video_ids():\n",
    "        return [doc['video_id'] for doc in labels_collection.find({}, {'video_id': 1})]\n",
    "\n",
    "def fetch_label(video_id):\n",
    "        doc = labels_collection.find_one({'video_id': video_id})\n",
    "        return doc['label'] if doc else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d6c45e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_graphs_from_neo4j(video_id):\n",
    "        with neo4j_driver.session() as session:\n",
    "            # Get all unique time steps\n",
    "            result = session.run(\"\"\"\n",
    "                MATCH (n:PoseNode {video_id: $video_id})\n",
    "                RETURN DISTINCT n.time_index AS timestep\n",
    "                ORDER BY timestep ASC\n",
    "            \"\"\", video_id=video_id)\n",
    "            time_steps = [record[\"timestep\"] for record in result]\n",
    "\n",
    "            graphs = []\n",
    "            for t in time_steps:\n",
    "                # Fetch nodes\n",
    "                node_query = session.run(\"\"\"\n",
    "                    MATCH (n:PoseNode {video_id: $video_id, time_index: $t})\n",
    "                    RETURN n.node_index AS idx, n.angle AS angle, n.time AS time\n",
    "                    ORDER BY idx\n",
    "                \"\"\", video_id=video_id, t=t)\n",
    "\n",
    "                node_data = []\n",
    "                time_value = 0\n",
    "                for record in node_query:\n",
    "                    node_data.append(float(record[\"angle\"]))\n",
    "                    time_value = float(record[\"time\"])\n",
    "\n",
    "                x = torch.tensor(node_data, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "                # Fetch edges\n",
    "                edge_query = session.run(\"\"\"\n",
    "                    MATCH (a:PoseNode {video_id: $video_id, time_index: $t})-[r:CONNECTED_TO]->(b:PoseNode)\n",
    "                    RETURN a.node_index AS src, b.node_index AS dst, r.weight AS weight\n",
    "                \"\"\", video_id=video_id, t=t)\n",
    "\n",
    "                edge_index = []\n",
    "                edge_attr = []\n",
    "                for record in edge_query:\n",
    "                    edge_index.append([int(record[\"src\"]), int(record[\"dst\"])])\n",
    "                    edge_attr.append([float(record[\"weight\"])])\n",
    "\n",
    "                edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "                edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "\n",
    "                graphs.append({\n",
    "                    \"edge_index\": edge_index,\n",
    "                    \"edge_attr\": edge_attr,\n",
    "                    \"angle_features\": x,\n",
    "                    \"time\": time_value,\n",
    "                    \"source_video\": video_id,\n",
    "                    \"label\": fetch_label(video_id),\n",
    "                    \"node_mapping\": {},  # Optional: mapping if you have remapped indices\n",
    "                    \"reverse_mapping\": {},\n",
    "                    \"node_features\": x.clone()\n",
    "                })\n",
    "            return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ffa79a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph_sequences_from_db():\n",
    "    video_ids = fetch_all_video_ids()\n",
    "    all_data = []\n",
    "\n",
    "    for vid in video_ids:\n",
    "        try:\n",
    "            graph_sequence = fetch_graphs_from_neo4j(vid)\n",
    "            if graph_sequence:\n",
    "                all_data.append(graph_sequence)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading video {vid}: {e}\")\n",
    "\n",
    "    print(f\"âœ… Loaded {len(all_data)} videos from DB\")\n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "db5c46a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "class FreeThrowDataset(Dataset):\n",
    "    def __init__(self, matrix_data):\n",
    "        self.data = matrix_data\n",
    "        # The angle nodes are automatically mapped in our enhanced processing\n",
    "        # But we can still keep track of which landmarks are associated with angles\n",
    "        self.angle_node_landmarks = [\n",
    "            mp_pose.PoseLandmark.LEFT_ELBOW.value,\n",
    "            mp_pose.PoseLandmark.RIGHT_ELBOW.value,\n",
    "            mp_pose.PoseLandmark.LEFT_SHOULDER.value,\n",
    "            mp_pose.PoseLandmark.RIGHT_SHOULDER.value,\n",
    "            mp_pose.PoseLandmark.LEFT_KNEE.value,\n",
    "            mp_pose.PoseLandmark.RIGHT_KNEE.value,\n",
    "            mp_pose.PoseLandmark.LEFT_HIP.value,\n",
    "            mp_pose.PoseLandmark.RIGHT_HIP.value\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.data[idx]\n",
    "        data_sequence = []\n",
    "        \n",
    "        for timestep in sequence:\n",
    "            # Check if timestep is a dictionary (as expected)\n",
    "            if not isinstance(timestep, dict):\n",
    "                raise TypeError(f\"Expected dictionary, got {type(timestep)}. Value: {timestep}\")\n",
    "                \n",
    "            try:\n",
    "                edge_index = timestep['edge_index']\n",
    "                edge_attr = timestep['edge_attr']\n",
    "                label = timestep['label']\n",
    "                angle_features = timestep['angle_features']\n",
    "                \n",
    "                # Convert label to tensor if it's not already\n",
    "                if not isinstance(label, torch.Tensor):\n",
    "                    y = torch.tensor([label], dtype=torch.float)\n",
    "                else:\n",
    "                    y = label\n",
    "                \n",
    "                # Create graph data object with already remapped indices and features\n",
    "                data = Data(\n",
    "                    x=angle_features,  # Use angle features as node features\n",
    "                    edge_index=edge_index,\n",
    "                    edge_attr=edge_attr,\n",
    "                    y=y,\n",
    "                    num_nodes=angle_features.size(0)\n",
    "                )\n",
    "                \n",
    "                # Store additional information for reference\n",
    "                data.original_to_new_mapping = timestep['node_mapping']\n",
    "                data.new_to_original_mapping = timestep['reverse_mapping']\n",
    "                data.positional_features = timestep['node_features']  # Store original position features\n",
    "                data.time = timestep['time']\n",
    "                data.source_video = timestep['source_video']\n",
    "                \n",
    "                data_sequence.append(data)\n",
    "            except KeyError as e:\n",
    "                # Print detailed error info for debugging\n",
    "                print(f\"KeyError: {e} not found in timestep. Available keys: {list(timestep.keys())}\")\n",
    "                raise\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing timestep: {e}\")\n",
    "                raise\n",
    "            \n",
    "        return data_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for batching sequences.\n",
    "    Each batch item is a sequence of frames, and we want to\n",
    "    maintain these sequences.\n",
    "    \"\"\"\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b2ae6874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_LSTM(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, lstm_hidden, num_classes):\n",
    "        super().__init__()\n",
    "        self.gcn = GCNConv(in_channels, hidden_channels)\n",
    "        self.lstm = nn.LSTM(hidden_channels, lstm_hidden, batch_first=True)\n",
    "        self.classifier = nn.Linear(lstm_hidden, num_classes)\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        gcn_outputs = []\n",
    "        for data in sequence:\n",
    "            x = self.gcn(data.x, data.edge_index)\n",
    "            x = torch.relu(x)\n",
    "            pooled = x.mean(dim=0)  # Global mean pooling\n",
    "            gcn_outputs.append(pooled)\n",
    "\n",
    "        gcn_outputs = torch.stack(gcn_outputs).unsqueeze(0)  # [1, T, F]\n",
    "        lstm_out, _ = self.lstm(gcn_outputs)\n",
    "        out = self.classifier(lstm_out[:, -1, :])  # Use last time step\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f293e3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 60 videos from DB\n"
     ]
    }
   ],
   "source": [
    "graphdata=load_graph_sequences_from_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5d4b0367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "68\n"
     ]
    }
   ],
   "source": [
    "print(len(graphdata))\n",
    "print(len(graphdata[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4cf14209",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(graphdata)\n",
    "split = int(0.7 * len(graphdata))\n",
    "train_matrix = graphdata[:split]\n",
    "test_matrix = graphdata[split:]\n",
    "\n",
    "train_dataset = FreeThrowDataset(train_matrix)\n",
    "test_dataset = FreeThrowDataset(test_matrix)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d86df277",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN_LSTM(in_channels=1, hidden_channels=32, lstm_hidden=16, num_classes=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e56790f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 28.9152\n",
      "Epoch 2 - Loss: 26.4335\n",
      "Epoch 3 - Loss: 26.1221\n",
      "Epoch 4 - Loss: 26.4773\n",
      "Epoch 5 - Loss: 26.0238\n",
      "Epoch 6 - Loss: 25.9259\n",
      "Epoch 7 - Loss: 25.9511\n",
      "Epoch 8 - Loss: 25.8912\n",
      "Epoch 9 - Loss: 25.6281\n",
      "Epoch 10 - Loss: 25.7117\n",
      "Epoch 11 - Loss: 25.6445\n",
      "Epoch 12 - Loss: 25.5260\n",
      "Epoch 13 - Loss: 25.6860\n",
      "Epoch 14 - Loss: 25.4818\n",
      "Epoch 15 - Loss: 25.5574\n",
      "Epoch 16 - Loss: 25.3836\n",
      "Epoch 17 - Loss: 26.0285\n",
      "Epoch 18 - Loss: 25.3294\n",
      "Epoch 19 - Loss: 25.3650\n",
      "Epoch 20 - Loss: 25.2722\n",
      "Epoch 21 - Loss: 25.3602\n",
      "Epoch 22 - Loss: 25.5061\n",
      "Epoch 23 - Loss: 25.4994\n",
      "Epoch 24 - Loss: 25.2227\n",
      "Epoch 25 - Loss: 25.2028\n",
      "Epoch 26 - Loss: 25.4472\n",
      "Epoch 27 - Loss: 25.1104\n",
      "Epoch 28 - Loss: 24.9300\n",
      "Epoch 29 - Loss: 25.1114\n",
      "Epoch 30 - Loss: 25.2619\n",
      "Epoch 31 - Loss: 24.9557\n",
      "Epoch 32 - Loss: 25.5159\n",
      "Epoch 33 - Loss: 25.6152\n",
      "Epoch 34 - Loss: 24.9046\n",
      "Epoch 35 - Loss: 25.0050\n",
      "Epoch 36 - Loss: 24.7620\n",
      "Epoch 37 - Loss: 25.3235\n",
      "Epoch 38 - Loss: 25.0255\n",
      "Epoch 39 - Loss: 24.6949\n",
      "Epoch 40 - Loss: 24.9693\n",
      "Epoch 41 - Loss: 25.0545\n",
      "Epoch 42 - Loss: 25.1446\n",
      "Epoch 43 - Loss: 24.7868\n",
      "Epoch 44 - Loss: 25.1239\n",
      "Epoch 45 - Loss: 24.7597\n",
      "Epoch 46 - Loss: 24.7790\n",
      "Epoch 47 - Loss: 24.9402\n",
      "Epoch 48 - Loss: 24.8639\n",
      "Epoch 49 - Loss: 24.8447\n",
      "Epoch 50 - Loss: 24.7475\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        sequence = batch[0]  # batch size = 1\n",
    "        target = sequence[0].y\n",
    "        output = model(sequence)\n",
    "        loss = loss_fn(output.view(-1), target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "329d4b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 16/18 = 88.89%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        sequence = batch[0]\n",
    "        target = int(sequence[0].y.item())\n",
    "        output = model(sequence)\n",
    "        prediction = (torch.sigmoid(output) > 0.5).int().item()\n",
    "        correct += int(prediction == target)\n",
    "        total += 1\n",
    "\n",
    "print(f\"Test Accuracy: {correct}/{total} = {correct / total:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
