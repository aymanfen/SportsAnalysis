{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d602f2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.structures import Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2d65654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Setup Detectron2\n",
    "def setup_detectron2():\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"))\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\")\n",
    "    cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "    metadata = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])\n",
    "    return predictor, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "220d833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Manual ROI selection\n",
    "def select_roi(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    if not ret:\n",
    "        raise Exception(\"Failed to read video\")\n",
    "\n",
    "    bbox = cv2.selectROI(\"Select Player Area\", frame, fromCenter=False, showCrosshair=True)\n",
    "    cv2.destroyWindow(\"Select Player Area\")\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "176b1340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Score function based on proximity to selected ROI\n",
    "def score_by_proximity(box, selected_box):\n",
    "    sx, sy, sw, sh = selected_box\n",
    "    sel_cx, sel_cy = sx + sw / 2, sy + sh / 2\n",
    "    x1, y1, x2, y2 = box\n",
    "    box_cx = (x1 + x2) / 2\n",
    "    box_cy = (y1 + y2) / 2\n",
    "    return -np.sqrt((box_cx - sel_cx) ** 2 + (box_cy - sel_cy) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80e04b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Main pipeline\n",
    "def extract_keypoints_from_roi(input_video_path, output_npz_path, output_video_path):\n",
    "    predictor, metadata = setup_detectron2()\n",
    "    selected_box = select_roi(input_video_path)\n",
    "\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    out_vid = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (width, height))\n",
    "\n",
    "    all_keypoints = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        outputs = predictor(frame)\n",
    "        instances = outputs[\"instances\"].to(\"cpu\")\n",
    "\n",
    "        if len(instances) == 0:\n",
    "            out_vid.write(frame)\n",
    "            continue\n",
    "\n",
    "        boxes = instances.pred_boxes.tensor.numpy()\n",
    "        keypoints = instances.pred_keypoints.numpy()\n",
    "\n",
    "        scores = [score_by_proximity(box, selected_box) for box in boxes]\n",
    "        best_idx = int(np.argmax(scores))\n",
    "        selected_kpts = keypoints[best_idx][:, :2]  # drop confidence\n",
    "\n",
    "        all_keypoints.append(selected_kpts)\n",
    "\n",
    "        # Draw only the selected person\n",
    "        single_instance = Instances(image_size=frame.shape[:2])\n",
    "        single_instance.pred_boxes = instances.pred_boxes[[best_idx]]\n",
    "        single_instance.pred_keypoints = instances.pred_keypoints[[best_idx]]\n",
    "        single_instance.scores = instances.scores[[best_idx]]\n",
    "        single_instance.pred_classes = instances.pred_classes[[best_idx]]\n",
    "\n",
    "        vis = Visualizer(frame[:, :, ::-1], metadata=metadata, scale=1.0)\n",
    "        vis_frame = vis.draw_instance_predictions(single_instance)\n",
    "        result = vis_frame.get_image()[:, :, ::-1]\n",
    "\n",
    "        out_vid.write(result)\n",
    "\n",
    "    cap.release()\n",
    "    out_vid.release()\n",
    "\n",
    "    np.savez_compressed(output_npz_path, keypoints=np.array(all_keypoints))\n",
    "    print(f\"Saved keypoints to: {output_npz_path}\")\n",
    "    print(f\"Saved video to: {output_video_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "008da112",
   "metadata": {},
   "outputs": [],
   "source": [
    "video='Clip14Miss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88b1b158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved keypoints to: data/custom/Clip14Miss.npz\n",
      "Saved video to: outputdir/NewClip14Miss.mp4\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "extract_keypoints_from_roi(f'inputdir/{video}.mp4',f'data/custom/{video}.npz',f'outputdir/New{video}.mp4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
